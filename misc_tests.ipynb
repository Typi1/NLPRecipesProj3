{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Typic\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 64.4MB/s]                    \n",
      "2023-03-12 05:14:44 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-03-12 05:14:46 INFO: File exists: C:\\Users\\Typic\\stanza_resources\\en\\default.zip\n",
      "2023-03-12 05:14:50 INFO: Finished downloading models and saved to C:\\Users\\Typic\\stanza_resources.\n",
      "2023-03-12 05:14:50 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 32.2MB/s]                    \n",
      "2023-03-12 05:14:51 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-03-12 05:14:51 INFO: Use device: cpu\n",
      "2023-03-12 05:14:51 INFO: Loading: tokenize\n",
      "2023-03-12 05:14:51 INFO: Loading: pos\n",
      "2023-03-12 05:14:52 INFO: Loading: lemma\n",
      "2023-03-12 05:14:52 INFO: Loading: depparse\n",
      "2023-03-12 05:14:52 INFO: Loading: sentiment\n",
      "2023-03-12 05:14:53 INFO: Loading: constituency\n",
      "2023-03-12 05:14:53 INFO: Loading: ner\n",
      "2023-03-12 05:14:54 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza as st\n",
    "import numpy as np\n",
    "import re as re\n",
    "from typing import Optional\n",
    "import transformers as tra\n",
    "\n",
    "st.download('en')\n",
    "depgram = st.Pipeline('en')#, processors='tokenize,mwt,pos,lemma,depparse,ner')\n",
    "pipe2 = tra.pipeline(model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if head_id is -1 or id is 0, this is the root node\n",
    "class depNode():\n",
    "    def __init__(self, id:int, head_id:int, text:str, typ: str, deps:dict):\n",
    "        self.id = id\n",
    "        self.head_id = head_id\n",
    "        self.text = text\n",
    "        self.typ = typ\n",
    "        self.deps = deps\n",
    "        if deps == None:\n",
    "            self.deps = {}\n",
    "\n",
    "    def addDependent(self, dep_id:int, dep_text:str, rel_type:str):\n",
    "        if dep_id in self.deps.keys():\n",
    "            return\n",
    "        self.deps[dep_id] = (dep_id, dep_text, rel_type)\n",
    "\n",
    "    def __str__(self):\n",
    "        outputStr = \"\"\n",
    "        outputStr += \"ID: \" + str(self.id)\n",
    "        outputStr += \"\\n\\tHead ID: \" + str(self.head_id)\n",
    "        outputStr += \"\\n\\tText: \" + self.text\n",
    "        outputStr += \"\\n\\tType: \" + self.typ\n",
    "        outputStr += \"\\n\\tDependent Words: \"\n",
    "        for x in self.deps.keys():\n",
    "            outputStr += \"\\n\\t\\tDep. ID: \" + str(self.deps[x][0])\n",
    "            outputStr += \"\\n\\t\\t\\tDep. Text: \" + self.deps[x][1]\n",
    "            outputStr += \"\\n\\t\\t\\tRelation Type: \" + self.deps[x][2]\n",
    "        return outputStr\n",
    "            \n",
    "\n",
    "# returns a easier to traverse dependency tree with word id in the sentence as the key, as well as a dictionary of a word to a list of ids of instances of it\n",
    "def getDependency(input_dep:list):\n",
    "    text_to_ids = {} # ex: For the sentence, \"cakes are cakes\": text_to_ids[\"cakes\"] == [1,3] \n",
    "    dependency_dict = {}\n",
    "\n",
    "    # add a special node for ROOT\n",
    "    dependency_dict[0] = depNode(0, -1, \"ROOT\", \"N/A\", {})\n",
    "\n",
    "    for entry in input_dep:\n",
    "\n",
    "        id1 = entry[0].id\n",
    "        txt1 = entry[0].text.lower()\n",
    "        id2 = entry[2].id\n",
    "        txt2 = entry[2].text.lower()\n",
    "        rel_type = entry[1]\n",
    "\n",
    "        # if either word id isn't in the dependency dictionary, add it\n",
    "        if not id1 in dependency_dict.keys():\n",
    "            dependency_dict[id1] = depNode(id1, entry[0].head, txt1, entry[0].xpos, {})\n",
    "            if not txt1 in text_to_ids.keys():\n",
    "                text_to_ids[txt1] = [id1]\n",
    "            elif txt1 in text_to_ids.keys() and not id1 in text_to_ids[txt1]:\n",
    "                text_to_ids[txt1] = text_to_ids[txt1] + [id1]\n",
    "\n",
    "        if not id2 in dependency_dict.keys():\n",
    "            dependency_dict[id2] = depNode(id2, entry[2].head, txt2, entry[2].xpos, {})\n",
    "            if not txt2 in text_to_ids.keys():\n",
    "                text_to_ids[txt2] = [id2]\n",
    "            elif txt2 in text_to_ids.keys() and not id2 in text_to_ids[txt2]:\n",
    "                text_to_ids[txt2] = text_to_ids[txt2] + [id2]\n",
    "\n",
    "        # add a dependency into the head word\n",
    "        dependency_dict[id1].addDependent(id2, txt2, rel_type)\n",
    "\n",
    "    return (dependency_dict, text_to_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes sentence parsing better through removing problematic verbs that make sentences more complex.\n",
    "# this should only be done if we know it isn't about the recipe or next/previous/current step!!!! it removes some stuff important to those\n",
    "def removeProblemWords(text:str):\n",
    "    final = text\n",
    "\n",
    "    let_result = re.search(\"(^|\\s)let\\W(s\\W)*\",text.lower())\n",
    "    if let_result != None:\n",
    "        final = (text[:let_result.span()[0]] + \" \" +  text[let_result.span()[1]:]).lstrip().rstrip()\n",
    "\n",
    "    can_result = re.search(\"(^|\\s)can\\W\",final.lower())\n",
    "    if can_result != None:\n",
    "        final = (final[:can_result.span()[0]] + \" \" +  final[can_result.span()[1]:]).lstrip().rstrip()\n",
    "    \n",
    "    know_result = re.search(\"(^|\\s)know\\W\",final.lower())\n",
    "    if know_result != None:\n",
    "        final = (final[:know_result.span()[0]] + \" \" +  final[know_result.span()[1]:]).lstrip().rstrip()\n",
    "\n",
    "    me_result = re.search(\"(^|\\s)me\\W\",final.lower())\n",
    "    if me_result != None:\n",
    "        final = (final[:me_result.span()[0]] + \" \" +  final[me_result.span()[1]:]).lstrip().rstrip()\n",
    "    \n",
    "    you_result = re.search(\"(^|\\s)you\\W\",final.lower())\n",
    "    if you_result != None:\n",
    "        final = (final[:you_result.span()[0]] + \" \" +  final[you_result.span()[1]:]).lstrip().rstrip()\n",
    "    \n",
    "    we_result = re.search(\"(^|\\s)we\\W\",final.lower())\n",
    "    if we_result != None:\n",
    "        final = (final[:we_result.span()[0]] + \" \" +  final[we_result.span()[1]:]).lstrip().rstrip()\n",
    "\n",
    "    love_result = re.search(\"(^|\\s)love(s|d)*\\W\",final.lower()) # evil programmer removes love\n",
    "    if love_result != None:\n",
    "        final = (final[:love_result.span()[0]] + \" \" +  final[love_result.span()[1]:]).lstrip().rstrip()\n",
    "\n",
    "    like_result = re.search(\"(^|\\s)like(s|d)*\\W\",final.lower())\n",
    "    if like_result != None:\n",
    "        final = (final[:like_result.span()[0]] + \" \" +  final[like_result.span()[1]:]).lstrip().rstrip()\n",
    "\n",
    "    step_result = re.search(\"(^|\\s)steps*\\W\",final.lower())\n",
    "    if step_result != None:\n",
    "        final = (final[:step_result.span()[0]] + \" \" +  final[step_result.span()[1]:]).lstrip().rstrip()\n",
    "\n",
    "    return final\n",
    "\n",
    "# returns a list with each entry corresponding to a dependent word on the head word provided\n",
    "# (word text, relation to head, word type, id in dependency dict)\n",
    "# ex: (\"it\", \"obj\", \"PRP\", 5)\n",
    "def getDepInfo(input_deps:dict, head:depNode):\n",
    "    res = []\n",
    "    for dd in head.deps:\n",
    "        text = head.deps[dd][1]\n",
    "        rel_type = head.deps[dd][2]\n",
    "        word_type = input_deps[head.deps[dd][0]].typ\n",
    "        res.append((text, rel_type, word_type, head.deps[dd][0]))\n",
    "    return res\n",
    "\n",
    "def isVague(text:str, typ:str):\n",
    "    text = text.lower()\n",
    "    if \"PRP\" in typ or \"DT\" in typ:\n",
    "        return True\n",
    "    if \"thing\" in text:\n",
    "        return True\n",
    "    if text == \"stuff\":\n",
    "        return True\n",
    "    if \"WP\" in typ or \"WRB\" in typ:\n",
    "        return True\n",
    "    if \"IN\" in typ:\n",
    "        return True\n",
    "    if \"JJ\" in typ:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def determineVaguenessFromDep(input_deps:dict):\n",
    "    head = getHeadWord(input_deps)\n",
    "    head_type = head.typ\n",
    "\n",
    "    print(head)\n",
    "    dep_list = getDepInfo(input_deps, head)\n",
    "\n",
    "    # condition based on head type\n",
    "    if \"VB\" in head_type:\n",
    "        # if it is a verb, then we check the object and nsubj, then maybe obl in that order\n",
    "        for dl in dep_list:\n",
    "            if \"obj\" in dl[1]:\n",
    "                # ok so if the object is vague, then the input could be vague but let's check nsubj first\n",
    "                if isVague(dl[0], dl[2]):\n",
    "                    break\n",
    "                # if the object isn't vague, then the input is absolutely being specific. The \"these\" case is handled outside of this function\n",
    "                else: \n",
    "                    return False\n",
    "        for dl in dep_list:\n",
    "            if \"nsubj\" in dl[1]:\n",
    "                # if the nsubj is vague, \n",
    "                if isVague(dl[0], dl[2]):\n",
    "                    break\n",
    "                # if the object isn't vague, then the input is absolutely being specific. The \"these\" case is handled outside of this function\n",
    "                else: \n",
    "                    return False\n",
    "    elif \"NN\" in head_type:\n",
    "        pass\n",
    "    elif head_type == \"WRB\":\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "    return head_type\n",
    "\n",
    "# wrapper for determineVaguenessFromDep, with a few easy exit cases\n",
    "# NOTE that vagueness doesn't really check how vague something is, but is more of a distinguisher between situations we would\n",
    "# call a google search or where we would check recipe steps.\n",
    "# so for example, \"What is it?\" would be an actually vague question that this says is vague.\n",
    "# However, \"How do I cut these strawberries?\" isn't actually vague by definition since it is being very specific, but it would fall under the same\n",
    "# type of procedure we would use for a vague step, where we check the instructions for info.\n",
    "# Something like \"How do I cut a strawberry?\" is kind of middle-of-the-road vagueness compared to the previous examples, but we would say it is being\n",
    "# specific since they aren't explicitly referring to the instructions (there may be some implicit intention there but ambiguity is hard).\n",
    "def determineVagueness(text:str):\n",
    "    # if \"these\" is in the text, then it must be referring to something specifically in relation to the recipe\n",
    "    # this is also an important exception to my other logic, bc \"strawberries\" is Q4 BUT \"these strawberries\" is Q3\n",
    "    if \"these\" in text.lower():\n",
    "        return True\n",
    "    \n",
    "    test_doc = depgram(removeProblemWords(text))\n",
    "    return determineVaguenessFromDep(getDependency(test_doc.sentences[0].dependencies))\n",
    "\n",
    "# returns the id, text, and type of the head word of the given dependency dict (in depNode form)\n",
    "def getHeadWord(input_deps:dict):\n",
    "    return input_deps[list(input_deps[0].deps.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ingredient:\n",
    "    def __init__(   self, \n",
    "                    og_text:str, # the original string\n",
    "                    main_comp:str, # the main part of the ingredient component, IE chicken\n",
    "                    quantity:str, # a number. if a non-numerical amount, this should be None (ex: some raisins). \"a\" -> 1. is a string rather than a float bc fractions are more readable for recipes\n",
    "                    measurement:str, # the measurement the quantity is referring to (like a cup). if no measurement (like \"2 apples\"), is None. if vague, non-committal amount (ex: \"some\"), this does here.\n",
    "                    sub_quantity:str, \n",
    "                    sub_measurement: str,\n",
    "                    descriptors:list # other details of the ingredient listing, IE dependent nouns, adjectives, preparation verb parts (ex: \"finely chopped\")\n",
    "                    ):\n",
    "        self.og_text = og_text\n",
    "        self.main_comp = main_comp\n",
    "        self.quantity = quantity\n",
    "        self.measurement = measurement\n",
    "        self.sub_quantity = sub_quantity\n",
    "        self.sub_measurement = sub_measurement\n",
    "        self.descriptors = descriptors\n",
    "\n",
    "    def __str__(self):\n",
    "        outputStr = \"\"\n",
    "        outputStr += \"Ingredient: \" + self.main_comp\n",
    "        outputStr += \"\\n\\tQuantity: \" \n",
    "        if self.quantity == None:\n",
    "            outputStr += \"N/A\"\n",
    "        else:\n",
    "            outputStr += self.quantity\n",
    "        outputStr += \"\\n\\tMeasurement: \" \n",
    "        if self.measurement == None:\n",
    "            if self.sub_quantity == None and self.sub_measurement == None:\n",
    "                outputStr += \"N/A\"\n",
    "            else:\n",
    "                outputStr += self.sub_quantity + \" \" + self.sub_measurement + \" \"\n",
    "        else:\n",
    "\n",
    "            if self.sub_quantity != None:\n",
    "                outputStr += self.sub_quantity + \" \" \n",
    "            if self.sub_measurement != None:\n",
    "                outputStr += self.sub_measurement + \" \"\n",
    "\n",
    "            outputStr += self.measurement\n",
    "        \n",
    "        outputStr += \"\\n\\tDescriptors: \"\n",
    "        if len(self.descriptors) < 1:\n",
    "            outputStr += \"\\n\\t\\tN/A\"\n",
    "        else:\n",
    "            for dt in self.descriptors:\n",
    "                outputStr += \"\\n\\t\\t\" + dt\n",
    "        outputStr += \"\\n\\tOriginal text: \" + self.og_text\n",
    "        return outputStr\n",
    "\n",
    "def combineItemsIntoPhrase(its:list):\n",
    "    res = \"\"\n",
    "    for x in its:\n",
    "        if x in [\".\", \",\", \"'\", \";\", \":\", \"-\", \"/\"] or (len(x) > 1 and x[0] in [\".\", \",\", \"'\", \";\", \":\", \"-\", \"/\"]):\n",
    "            res = res.rstrip()\n",
    "            \n",
    "        res += x\n",
    "        res += \" \"  \n",
    "    res = res.replace(\"&#39;\", \"'\")\n",
    "    return res.rstrip()\n",
    "\n",
    "def floatFromFractionString(frac: str):\n",
    "    try:\n",
    "        return float(frac)\n",
    "    except:\n",
    "        numerator = frac[:frac.index(\"/\")]\n",
    "        denominator = frac[frac.index(\"/\")+1:]\n",
    "\n",
    "        return float(numerator) / float(denominator)\n",
    "\n",
    "# tries to find a quantity and measurement if none were found by directly analyzing the head word\n",
    "def tryFindQuantity(input_deps:dict, head:depNode, head_rel_type:str, meas: str):\n",
    "    quantity = None\n",
    "    measurement = None\n",
    "\n",
    "    # if measurement, then set measurement\n",
    "    if head_rel_type == \"nmod:npmod\":\n",
    "        measurement = head.text\n",
    "\n",
    "    if head_rel_type in [\"nummod\", 'det']:\n",
    "        if head_rel_type == 'det' and head.text.lower() in [\"a\", \"an\"]:\n",
    "            # print(\"???\")\n",
    "            return (str(1), measurement)\n",
    "        elif head_rel_type == 'det' and meas != None:\n",
    "            # print(\"???\")\n",
    "            return (head.text, measurement)\n",
    "        elif head_rel_type == \"nummod\":\n",
    "            # print(\"???\")\n",
    "            return (head.text, measurement)\n",
    "\n",
    "    for dd in head.deps:\n",
    "        rel_type = head.deps[dd][2]\n",
    "        # print(head.deps[dd])\n",
    "        (temp_quantity, temp_measurement) = tryFindQuantity(input_deps, input_deps[head.deps[dd][0]], rel_type, measurement)\n",
    "        if temp_measurement != None:\n",
    "            measurement = temp_measurement\n",
    "        if temp_quantity != None:\n",
    "            # print(temp_quantity)\n",
    "            if quantity == None:\n",
    "                quantity = temp_quantity\n",
    "            else:\n",
    "                quantity == str(floatFromFractionString(quantity) * floatFromFractionString(temp_quantity))\n",
    "            if measurement == None:\n",
    "                measurement = head.text\n",
    "\n",
    "\n",
    "    return (quantity, measurement)\n",
    "\n",
    "def getIngredientParameters(depgram, ingred:str):\n",
    "    og_ingred = ingred\n",
    "    sub_phrase = None\n",
    "    sub_quantity = None\n",
    "    sub_measurement = None\n",
    "    after_comma = None\n",
    "    if \"(\" in ingred and \")\" in ingred:\n",
    "        sub_measurement_result = re.search(\"\\((.+)\\)\", ingred)\n",
    "        if sub_measurement_result != None:\n",
    "            sub_phrase = sub_measurement_result.group(1)\n",
    "            # print(sub_phrase)\n",
    "            ingred = ingred.replace(sub_phrase, \"\")\n",
    "            nums = re.search(\"\\s*([\\d/\\.]+)\\s*\", sub_phrase)\n",
    "            if nums != None:\n",
    "                sub_quantity = nums.group(1)\n",
    "                sub_measurement = sub_phrase[nums.span()[1]:]\n",
    "\n",
    "        # # ingred = ingred.replace(\"packages\", \"\")\n",
    "        # # ingred = ingred.replace(\"package\", \"\")\n",
    "        ingred = ingred.replace(\"(\", \"\")\n",
    "        ingred = ingred.replace(\")\", \"\")\n",
    "\n",
    "    if \",\" in ingred:\n",
    "        comma_result = re.search(\"\\,(.+)\", ingred)\n",
    "        if comma_result != None:\n",
    "            after_comma = comma_result.group(1)\n",
    "            ingred = ingred.replace(after_comma, \"\")\n",
    "            ingred = ingred.replace(\",\", \"\")\n",
    "\n",
    "        # remove_list = [\"package\", \"can\"]\n",
    "        \n",
    "        # for rr in remove_list:\n",
    "        #     temp_result = re.search(\"\\s(\" + rr + \"s*)\\s\", ingred)\n",
    "        #     if temp_result != None:\n",
    "        #         print(temp_result.span())\n",
    "        #         ingred = ingred[:temp_result.span()[0]] + \" \" + ingred[temp_result.span()[1]:]\n",
    "        #         # print(\"CANNN\")\n",
    "        \n",
    "        # nums = re.findall(\"\\s*([\\d/\\.]+)\\s*\", ingred)\n",
    "        # if nums != None:\n",
    "        #     print(nums)\n",
    "        #     nums.sort(key=lambda x: len(x), reverse=True)\n",
    "        #     product = 1\n",
    "        #     for nn in nums:\n",
    "        #         ingred = ingred.replace(nn, \"\")\n",
    "        #         product *= floatFromFractionString(nn)\n",
    "\n",
    "        #     ingred = str(product) + ingred\n",
    "        # print(ingred)\n",
    "\n",
    "    doc = depgram(ingred)\n",
    "\n",
    "    # consti = doc.sentences[0].constituency\n",
    "    depend = getDependency(doc.sentences[0].dependencies)[0]\n",
    "    head_dep = depend[list(depend[0].deps.keys())[0]]\n",
    "    # print(head_dep)\n",
    "    # print(consti)\n",
    "\n",
    "    # for dd in depend:\n",
    "    #     print(depend[dd])\n",
    "\n",
    "    # from what I can tell, the head noun will reference 4 common types of dependent relations:\n",
    "    #   1: \"amod\" | \"parataxis\": should go into the descriptors list [note that the parataxis is something weird where if there is a comma or something]\n",
    "    #   2: \"compound\": should be part of the main_comp (make sure to figure out how to enjoin with the head word)\n",
    "    #   3: \"nmod:npmod\": should be the measurement field. if it exists, we need to go there and get the nummod or a de (\"a\", \"some\", etc.)\n",
    "    #   4: \"nummod\": this should be the quantity field. if this is dependent on the root noun, then there is no measurement word, so set it to None\n",
    "    \n",
    "    descriptors = []\n",
    "    compounds = []\n",
    "    measurement = None\n",
    "    quantity = None\n",
    "    main_comp = head_dep.text\n",
    "    \n",
    "    deps = getDepInfo(depend, head_dep)\n",
    "    # print(deps)\n",
    "    \n",
    "    for dd in deps:\n",
    "        if dd[1] == 'nmod:npmod':\n",
    "            measurement = dd[0]\n",
    "            temp_quant = getDepInfo(depend, depend[dd[3]])\n",
    "            if temp_quant == None:\n",
    "                continue\n",
    "            for tq in temp_quant:\n",
    "                if tq[0] == 'no':\n",
    "                    continue\n",
    "                if tq[1] == 'nummod':\n",
    "                    quantity = tq[0]\n",
    "                elif tq[1] == 'det' and (tq[0].lower() == 'a' or tq[0].lower() == 'an'):\n",
    "                    quantity = str(1)\n",
    "                elif quantity == None:\n",
    "                    quantity = tq[0]\n",
    "                    # note no break here bc this is a failsafe in case there is no appropriate quantity descriptor\n",
    "        elif dd[1] == 'amod' or dd[1] == 'parataxis' or dd[1] == 'conj' or dd[1] == 'acl':\n",
    "            # also add certain dependents on the amod if they exist (ex: \"all-purpose flour\" has \"all\" as dependent on \"purpose\")\n",
    "            temp_text = dd[0]\n",
    "            temp_desc = getDepInfo(depend, depend[dd[3]])\n",
    "            temp_adds = []\n",
    "            if temp_desc != None:\n",
    "                for td in temp_desc:\n",
    "                    if td[1] in ['det', 'amod', 'obl']:\n",
    "                        temp_adds.append(td[0])\n",
    "                    elif td[1] in ['punct']:\n",
    "                        continue\n",
    "                    else:\n",
    "                        # print(\"What is this?? \" + str(td))\n",
    "                        continue\n",
    "            if len(temp_adds) > 0:\n",
    "                temp_adds = [temp_text] + temp_adds\n",
    "                temp_text = combineItemsIntoPhrase(temp_adds)\n",
    "            descriptors.append(temp_text)\n",
    "        elif dd[1] == 'compound' or dd[1] == 'appos' or dd[1] == 'aux':\n",
    "            remove_list = [\"package\", \"packages\", \"can\", \"cans\", \"jar\", \"jars\"]\n",
    "            if dd[0] in remove_list:\n",
    "                measurement = dd[0]\n",
    "            else:\n",
    "                # print(\">>>\" + str(depend[dd[3]]))\n",
    "                for abc in list(depend[dd[3]].deps.keys()):\n",
    "                    # print(depend[dd[3]].deps[abc])\n",
    "                    # print(depend[abc])\n",
    "                    if depend[dd[3]].deps[abc][2] == 'amod':\n",
    "                        compounds.append(depend[abc].text)\n",
    "                # print(list(depend[dd[3]].deps.keys()))\n",
    "                compounds.append(dd[0])\n",
    "        elif dd[1] == 'nummod':\n",
    "            quantity = dd[0]\n",
    "        elif dd[1] == 'det':\n",
    "            if dd[0].lower() == 'a' or dd[0].lower() == 'an':\n",
    "                quantity = str(1)\n",
    "            else:\n",
    "                if dd[0].lower() == 'no':\n",
    "                    compounds.append(dd[0])\n",
    "                    continue\n",
    "                quantity = dd[0]\n",
    "        elif dd[1] in ['aux', 'punct']:\n",
    "            continue\n",
    "        else:\n",
    "            print(\"??????\")\n",
    "            # descriptors.append(dd[0])\n",
    "            print(dd)\n",
    "\n",
    "    if quantity == None and measurement == None:\n",
    "        # do something to try and find something because we should have at least 1\n",
    "\n",
    "        # first, let's try going through each of the compounds recursively with depth-first\n",
    "        # temp_quant = None\n",
    "        # temp_meas = None\n",
    "        for cc in deps:\n",
    "            (temp_quant, temp_meas) = tryFindQuantity(depend, depend[cc[3]], None, None)\n",
    "            if quantity == None and temp_quant != None:\n",
    "                quantity = temp_quant\n",
    "                \n",
    "            if measurement == None and temp_meas != None:\n",
    "                measurement = temp_meas\n",
    "                \n",
    "            if quantity != None and measurement != None:\n",
    "                break\n",
    "\n",
    "\n",
    "    if len(compounds) > 0:\n",
    "        compounds.append(main_comp)\n",
    "        main_comp = combineItemsIntoPhrase(compounds)\n",
    "\n",
    "    if after_comma != None:\n",
    "        descriptors.append(after_comma.lstrip().rstrip())\n",
    "\n",
    "    return Ingredient(og_ingred, main_comp, quantity, measurement, sub_quantity, sub_measurement, descriptors)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredient: mascarpone cheese\n",
      "\tQuantity: 1\n",
      "\tMeasurement: 8 ounce container\n",
      "\tDescriptors: \n",
      "\t\tsoftened\n",
      "\tOriginal text: 1 (8 ounce) container mascarpone cheese, softened\n"
     ]
    }
   ],
   "source": [
    "test_phrases = ['1 cup butter, softened', '1 cup white sugar', '1 cup packed brown sugar', '2 eggs', '2 teaspoons vanilla extract', '1 teaspoon baking soda', '2 teaspoons hot water', '0.5 teaspoon salt', '3 cups all-purpose flour', '2 cups semisweet chocolate chips', 'a cup chopped walnuts', 'an orange']\n",
    "test_phrases = ['1 tablespoon olive oil', '1 small onion, diced', '4 cloves garlic, minced', '1.5 pounds ground beef', '1 teaspoon garlic powder', '1 (28 ounce) jar sausage flavored spaghetti sauce', '1 (8 ounce) can tomato sauce', '1 teaspoon dried oregano', '1 (8 ounce) package shredded mozzarella cheese', '1 (8 ounce) package shredded provolone cheese', '1 (15 ounce) container ricotta cheese', '0.25 cup milk', '2 large eggs', '0.5 teaspoon dried oregano', '9 no-boil lasagna noodles', '0.25 cup grated Parmesan cheese']\n",
    "test_phrases = ['2 (7 ounce) packages shirataki noodles, drained', '1 tablespoon vegetable oil', '1 (12 ounce) package tofu, cut into chunks', '0.25 cup reduced-sodium soy sauce', '0.5 cup lemon juice', '0.25 cup white sugar', '2 tablespoons peanut butter', '1 tablespoon sriracha hot sauce', '2 eggs', '1 (4.5 ounce) can mushrooms', '0.5 cup chopped cashews, divided', '1 cup bean sprouts', '1 lime, cut into wedges']\n",
    "test_phrases = ['3 cups Burgundy wine', '2 onions, thinly sliced', '2 carrots, chopped', '2 tablespoons brandy', '1 clove garlic, crushed', '10 whole black peppercorns', '1 teaspoon salt', '1 sprig fresh parsley', '1 bay leaf', '2 pounds cubed beef chuck roast', '4 tablespoons olive oil, divided', '0.25 pound bacon, cubed', '2 onions, chopped', '3 tablespoons all-purpose flour', '2 cloves garlic, crushed', '1 tablespoon tomato paste', '1 (10.5 ounce) can beef broth', 'salt and pepper to taste', '4 tablespoons butter', '1 pound fresh mushrooms, sliced']\n",
    "test_phrases = ['0.25 cup sesame oil', '0.25 cup lemon juice', '0.25 cup soy sauce', '2 tablespoons brown sugar, or more to taste', '1 tablespoon sesame seeds', '1 teaspoon ground mustard', '1 teaspoon ground ginger', '0.25 teaspoon garlic powder', '4 (6 ounce) salmon steaks']\n",
    "test_phrases = ['2 tablespoons olive oil', '1 carrot, diced', '0.5 green bell pepper, diced', '2 cups shrimp, peeled and deveined', '0.5 onion, diced', '0.5 (15.25 ounce) can whole kernel corn, drained', '2 cloves garlic, thinly sliced', '1 tablespoon olive oil', '2 eggs, beaten', '4 cups cooked rice, cooled - or more to taste', '2 tablespoons oyster sauce, or more to taste', '2 tablespoons soy sauce', '1 tablespoon butter', '0.5 teaspoon salt', '1 teaspoon butter, or as needed', '4 eggs, divided']\n",
    "test_phrases = ['1 (12 ounce) package ladyfingers', '0.25 cup unsalted butter, melted', '2 tablespoons coffee-flavored liqueur', '3 (8 ounce) packages cream cheese, softened', '1 (8 ounce) container mascarpone cheese, softened', '1 cup white sugar', '2 tablespoons coffee-flavored liqueur', '0.25 cup all-purpose flour', '2 large eggs', '1 teaspoon heavy cream, or more as needed', '0.25 ounce semisweet chocolate'] \n",
    "test_phrases = ['1 tablespoon vegetable oil', '1 cup long grain white rice', '1.5 cups chicken broth', '1 tomato, seeded and chopped', '0.5 onion, finely chopped', '0.5 green bell pepper, finely chopped', '1 fresh jalapeno pepper, chopped', '0.5 cup chopped fresh cilantro', '1 cube chicken bouillon', '1 clove garlic, halved', '0.5 teaspoon ground cumin', 'salt and pepper to taste']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# test_phrases = ['1 (28 ounce) jar sausage flavored spaghetti sauce', '1 (8 ounce) can tomato sauce', '1 (8 ounce) package shredded mozzarella cheese', '1 (8 ounce) package shredded provolone cheese', '1 (15 ounce) container ricotta cheese']\n",
    "# test_phrases = ['1 (12 ounce) package tofu, cut into chunks']\n",
    "test_phrases = ['1 (8 ounce) container mascarpone cheese, softened']\n",
    "# test_doc = de0.25 cup reduced-sodium soy saucepgram(removeProblemWords(test_phrase))\n",
    "\n",
    "# print(test_doc.sentences[0].constituency, 0)\n",
    "# print(test_doc.sentences[0].constituency, 0)\n",
    "# depe = getDependency(test_doc.sentences[0].dependencies)\n",
    "# for dd in depe[0]:\n",
    "#     print(depe[0][dd])\n",
    "\n",
    "# # removeProblemWords(test_phrase)\n",
    "# getHeadWord(depe[0])\n",
    "# determineVaguenessFromDep(depe[0])\n",
    "\n",
    "# print(type([\"hiya\"]))\n",
    "ingredients = []\n",
    "\n",
    "for tp in test_phrases:\n",
    "    and_result = re.search(\"(.+)\\sand\\s(.+)\", tp)\n",
    "    if and_result != None:\n",
    "        ingredients.append(getIngredientParameters(depgram, and_result.group(1).lstrip().rstrip()))\n",
    "        tp = and_result.group(2).lstrip().rstrip()\n",
    "    ingredients.append(getIngredientParameters(depgram, tp))\n",
    "\n",
    "for ing in ingredients:\n",
    "    print(ing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "def floatFromFractionString(frac: str):\n",
    "    try:\n",
    "        return float(frac)\n",
    "    except:\n",
    "        numerator = frac[:frac.index(\"/\")]\n",
    "        denominator = frac[frac.index(\"/\")+1:]\n",
    "\n",
    "        return float(numerator) / float(denominator)\n",
    "\n",
    "print(floatFromFractionString(\"1/2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
